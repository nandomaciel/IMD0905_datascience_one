{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson 17 - Data Cleaning Walkthrough_ combining the data.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Oi0OXsx48yFX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1.0 Data Cleaning Walkthrough: Combining the Data"
      ]
    },
    {
      "metadata": {
        "id": "H2CF1nWo8yFZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Introduction"
      ]
    },
    {
      "metadata": {
        "id": "Z2x3NcmX8yFa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the last mission, we began investigating possible relationships between **SAT scores** and **demographic factors**. In order to do this, we acquired several data sets about [New York City public schools](https://data.cityofnewyork.us/data?cat=education). We manipulated these data sets, and found that we could combine them all using the **DBN** column. All of the data sets are currently stored as **keys** in the **data** dictionary. Each individual data set is a pandas dataframe.\n",
        "\n",
        "In this section, **we'll clean the data a bit more**, then **combine** it. Finally, we'll **compute correlations** and perform some analysis.\n",
        "\n",
        "The first thing we'll need to do in preparation for the merge is condense some of the data sets. In the last section, we noticed that the values in the **DBN** column were unique in the **sat_results** data set. Other data sets like **class_size** had duplicate **DBN** values, however.\n",
        "\n",
        "We'll need to condense these data sets so that each value in the **DBN** column is unique. If not, we'll run into issues when it comes time to combine the data sets.\n",
        "\n",
        "While the main data set we want to analyze, **sat_results**, has unique **DBN** values for every high school in New York City, other data sets aren't as clean. A single row in the **sat_results** data set may match multiple rows in the **class_size** data set, for example. This situation will create problems, because we don't know which of the multiple entries in the **class_size** data set we should combine with the single matching entry in **sat_results**. Here's a diagram that illustrates the problem:\n",
        "\n",
        "\n",
        "<left><img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1deYm5RdQXO2xMX6dUgHLvqDEWipk3axq\"></left>\n",
        "\n",
        "In the diagram above, we can't just combine the rows from both data sets because there are several cases where multiple rows in **class_size** match a single row in **sat_results.**\n",
        "\n",
        "To resolve this issue, we'll condense the **class_size**, **graduation**, and **demographics** data sets so that each **DBN** is unique."
      ]
    },
    {
      "metadata": {
        "id": "L46dQU968yFb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Condensing the Class Size Data Set"
      ]
    },
    {
      "metadata": {
        "id": "yRCu0U0-8yFc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first data set that we'll condense is **class_size**. The first few rows of **class_size** look like this:\n",
        "\n",
        "|__| CSD | BOROUGH | SCHOOL CODE | SCHOOL NAME               | GRADE | PROGRAM TYPE | CORE SUBJECT (MS CORE and 9-12 ONLY) | CORE COURSE (MS CORE and 9-12 ONLY) | SERVICE CATEGORY(K-9* ONLY) | NUMBER OF STUDENTS / SEATS FILLED | NUMBER OF SECTIONS |\n",
        "|---|-----|---------|-------------|---------------------------|-------|--------------|--------------------------------------|-------------------------------------|-----------------------------|-----------------------------------|--------------------|\n",
        "| 0 | 1   | M       | M015        | P.S. 015 Roberto Clemente | 0K    | GEN ED       | -                                    | -                                   | -                           | 19.0                              | 1.0                |\n",
        "| 1 | 1   | M       | M015        | P.S. 015 Roberto Clemente | 0K    | CTT          | -                                    | -                                   | -                           | 21.0                              | 1.0                |\n",
        "| 2 | 1   | M       | M015        | P.S. 015 Roberto Clemente | 01    | GEN ED       | -                                    | -                                   | -                           | 17.0                              | 1.0                |\n",
        "| 3 | 1   | M       | M015        | P.S. 015 Roberto Clemente | 01    | CTT          | -                                    | -                                   | -                           | 17.0                              | 1.0                |\n",
        "| 4 | 1   | M       | M015        | P.S. 015 Roberto Clemente | 02    | GEN ED       | -                                    | -                                   | -                           | 15.0                              | 1.0                |\n",
        "\n",
        "As you can see, the first few rows all pertain to the same school, which is why the **DBN** appears more than once. It looks like each school has multiple values for **GRADE**, **PROGRAM TYPE**, **CORE SUBJECT (MS CORE and 9-12 ONLY)**, and **CORE COURSE (MS CORE and 9-12 ONLY)**.\n",
        "\n",
        "If we look at the unique values for **GRADE**, we get the following:\n",
        "\n",
        "```python\n",
        "array(['0K', '01', '02', '03', '04', '05', '0K-09', nan, '06', '07', '08',\n",
        "       'MS Core', '09-12', '09'], dtype=object)\n",
        "```\n",
        "\n",
        "Because we're dealing with high schools, we're only concerned with grades 9 through 12. That means we only want to pick rows where the value in the **GRADE** column is **09-12**.\n",
        "\n",
        "If we look at the unique values for **PROGRAM TYPE**, we get the following:\n",
        "\n",
        "```python\n",
        "array(['GEN ED', 'CTT', 'SPEC ED', nan, 'G&T'], dtype=object)\n",
        "```\n",
        "\n",
        "Each school can have multiple program types. Because **GEN ED** is the largest category by far, let's only select rows where **PROGRAM TYPE** is **GEN ED**.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gL1mmbAK8yFc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.3 Condensing the Class Size Data Set"
      ]
    },
    {
      "metadata": {
        "id": "8Zqa50Z08yFd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
        "\n",
        "\n",
        "- Create a new variable called **class_size**, and assign the value of **data[\"class_size\"]** to it.\n",
        "- Filter **class_size** so the **GRADE** column only contains the value **09-12.** Note that the name of the **GRADE** column has a space at the end; you'll generate an error if you don't include it.\n",
        "- Filter **lass_size** so that the **PROGRAM TYPE** column only contains the value **GEN ED.**\n",
        "- Display the first five rows of **class_size** to verify."
      ]
    },
    {
      "metadata": {
        "id": "8S8Ho7Tz8yFd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Th72ZVIj8yFf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.4 Computing Average Class Sizes"
      ]
    },
    {
      "metadata": {
        "id": "00ydSvyt8yFg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we saw when we displayed **class_size** on the last screen, **DBN** still isn't completely unique. This is due to the **CORE COURSE (MS CORE and 9-12 ONLY)** and **CORE SUBJECT (MS CORE and 9-12 ONLY)** columns.\n",
        "\n",
        "**CORE COURSE (MS CORE and 9-12 ONLY)** and **CORE SUBJECT (MS CORE and 9-12 ONLY)** seem to pertain to different kinds of classes. For example, here are the unique values for **CORE SUBJECT (MS CORE and 9-12 ONLY)**:\n",
        "\n",
        "```python\n",
        "array(['ENGLISH', 'MATH', 'SCIENCE', 'SOCIAL STUDIES'], dtype=object)\n",
        "```\n",
        "\n",
        "This column only seems to include certain subjects. We want our class size data to include every single class a school offers -- not just a subset of them. What we can do is take the average across all of the classes a school offers. This will give us unique **DBN** values, while also incorporating as much data as possible into the average.\n",
        "\n",
        "Fortunately, we can use the [pandas.DataFrame.groupby()](http://pandas.pydata.org/pandas-docs/stable/groupby.html) method to help us with this. The **DataFrame.groupby()** method will split a dataframe up into unique groups, based on a given column. We can then use the **agg()** method on the resulting **pandas.core.groupby** object to find the **mean** of each column.\n",
        "\n",
        "Let's say we have this data set:\n",
        "\n",
        "<left><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1sJjENlTRR56RwYzBmmsU8aIMELgjx8zg\"></left>\n",
        "\n",
        "Using the **groupby()** method, we'll split this dataframe into four separate groups -- one with the **DBN 01M292**, one with the **DBN 01M332**, one with the **DBN 01M378**, and one with the **DBN 01M448**:\n",
        "\n",
        "<left><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1y9imbMLKRDI50wQqPn7P6TAd6MfCL4Nq\"></left>\n",
        "\n",
        "<left><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1FitnyClxHDQLnoAB3jR7YI_jEPZZhkco\"></left>\n",
        "\n",
        "Then, we can compute the averages for the **AVERAGE CLASS SIZE** column in each of the four groups using the **agg()** method:\n",
        "\n",
        "<left><img width=\"200\" src=\"https://drive.google.com/uc?export=view&id=1gHVZixGOuGYYON_zU0OUPTJcC9Q_mKeV\"></left>\n",
        "\n",
        "After we group a dataframe and aggregate data based on it, the column we performed the grouping on (in this case **DBN**) will become the index, and will no longer appear as a column in the data itself. To undo this change and keep **DBN** as a column, we'll need to use [pandas.DataFrame.reset_index()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html). This method will reset the index to a list of integers and make **DBN** a column again."
      ]
    },
    {
      "metadata": {
        "id": "mMPOOzjN8yFg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.5 Computing Average Class Sizes"
      ]
    },
    {
      "metadata": {
        "id": "sfdxxqf08yFh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Find the average values for each column associated with each **DBN** in **class_size**.\n",
        "    - Use the [pandas.DataFrame.groupby()](http://pandas.pydata.org/pandas-docs/stable/groupby.html) method to group **class_size** by **DBN**.\n",
        "    - Use the [agg()](http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation) method on the resulting **pandas.core.groupby** object, along with the **numpy.mean()** function as an argument, to calculate the average of each group.\n",
        "    - Assign the result back to **class_size**.\n",
        "- Reset the index to make **DBN** a column again.\n",
        "    - Use the [pandas.DataFrame.reset_index()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html) method, along with the keyword argument **inplace=True**.\n",
        "- Assign **class_size** back to the **class_size** key of the **data** dictionary.\n",
        "- Display the first few rows of **data[\"class_size\"]** to verify that everything went okay."
      ]
    },
    {
      "metadata": {
        "id": "xGKLTHOi8yFi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W4XtGPNh8yFl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.6 Condensing the Demographics Data Set"
      ]
    },
    {
      "metadata": {
        "id": "ijDhJZeb8yFl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we've finished condensing **class_size**, let's condense **demographics**. The first few rows look like this:\n",
        "\n",
        "| _| DBN    | Name                      | schoolyear | fl_percent | frl_percent | total_enrollment | prek | k  | grade1 | grade2 |\n",
        "|---|--------|---------------------------|------------|------------|-------------|------------------|------|----|--------|--------|\n",
        "| 0 | 01M015 | P.S. 015 ROBERTO CLEMENTE | 20052006   | 89.4       | NaN         | 281              | 15   | 36 | 40     | 33     |\n",
        "| 1 | 01M015 | P.S. 015 ROBERTO CLEMENTE | 20062007   | 89.4       | NaN         | 243              | 15   | 29 | 39     | 38     |\n",
        "| 2 | 01M015 | P.S. 015 ROBERTO CLEMENTE | 20072008   | 89.4       | NaN         | 261              | 18   | 43 | 39     | 36     |\n",
        "| 3 | 01M015 | P.S. 015 ROBERTO CLEMENTE | 20082009   | 89.4       | NaN         | 252              | 17   | 37 | 44     | 32     |\n",
        "| 4 | 01M015 | P.S. 015 ROBERTO CLEMENTE | 20092010   |  _          | 96.5        | 208              | 16   | 40 | 28     | 32     |\n",
        "\n",
        "In this case, the only column that prevents a given **DBN** from being unique is **schoolyear**. We only want to select rows where schoolyear is **20112012**. This will give us the most recent year of data, and also match our SAT results data."
      ]
    },
    {
      "metadata": {
        "id": "VnDEAeuB8yFn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.7 Condensing the Demographics Data Set"
      ]
    },
    {
      "metadata": {
        "id": "C5g2PiJt8yFn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
        "\n",
        "- Filter **demographics**, only selecting rows in **data[\"demographics\"]** where **schoolyear** is **20112012.**\n",
        "    - **schoolyear** is actually an integer, so be careful about how you perform your comparison.\n",
        "- Display the first few rows of **data[\"demographics\"]** to verify that the filtering worked."
      ]
    },
    {
      "metadata": {
        "id": "4QUcmKI28yFo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x5dkSi8s8yFs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.8 Condensing the Graduation Data Set"
      ]
    },
    {
      "metadata": {
        "id": "5xQzgi_Q8yFs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we'll need to condense the **graduation** data set. Here are the first few rows:\n",
        "\n",
        "| _ | Demographic  | DBN    | School Name                           | Cohort   | Total Cohort | Total Grads - n |\n",
        "|---|--------------|--------|---------------------------------------|----------|--------------|-----------------|\n",
        "| 0 | Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2003     | 5            | s               |\n",
        "| 1 | Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2004     | 55           | 37              |\n",
        "| 2 | Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2005     | 64           | 43              |\n",
        "| 3 | Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2006     | 78           | 43              |\n",
        "| 4 | Total Cohort | 01M292 | HENRY STREET SCHOOL FOR INTERNATIONAL | 2006 Aug | 78           | 44              |\n",
        "\n",
        "The **Demographic** and **Cohort** columns are what prevent **DBN** from being unique in the **graduation** data. A **Cohort** appears to refer to the year the data represents, and the **Demographic** appears to refer to a specific demographic group. In this case, we want to pick data from the most recent Cohort available, which is 2006. We also want data from the full cohort, so we'll only pick rows where **Demographic** is **Total Cohort**."
      ]
    },
    {
      "metadata": {
        "id": "IIHasD3G8yFt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.9 Condensing the Graduation Data Set"
      ]
    },
    {
      "metadata": {
        "id": "7m8fZlxr8yFv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
        "\n",
        "- Filter **graduation**, only selecting rows where the **Cohort** column equals **2006.**\n",
        "- Filter **graduation**, only selecting rows where the **Demographic** column equals **Total Cohort**.\n",
        "- Display the first few rows of **data[\"graduation\"]** to verify that everything worked properly."
      ]
    },
    {
      "metadata": {
        "id": "JjMUikQu8yFv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ak0EfoSm8yFy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.10 Converting AP Test Scores"
      ]
    },
    {
      "metadata": {
        "id": "DMmk__7t8yFy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're almost ready to combine all of the data sets. The only remaining thing to do is convert the [Advanced Placement (AP)](https://en.wikipedia.org/wiki/Advanced_Placement_exams) test scores from strings to numeric values. High school students take the AP exams before applying to college. There are several AP exams, each corresponding to a school subject. High school students who earn high scores may receive college credit.\n",
        "\n",
        "AP exams have a 1 to 5 scale; 3 or higher is a passing score. Many high school students take AP exams -- particularly those who attend academically challenging institutions. AP exams are much more rare in schools that lack funding or academic rigor.\n",
        "\n",
        "It will be interesting to find out whether AP exam scores are correlated with SAT scores across high schools. To determine this, we'll need to convert the AP exam scores in the **ap_2010** data set to numeric values first.\n",
        "\n",
        "There are three columns we'll need to convert:\n",
        "\n",
        "- **AP Test Takers** (note that there's a trailing space in the column name)\n",
        "- **Total Exams Taken**\n",
        "- **Number of Exams with scores 3 4 or 5**\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
        "\n",
        "- Convert each of the following columns in **ap_2010** to numeric values using the [pandas.to_numeric()](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.to_numeric.html) function with the keyword argument **errors=\"coerce\".**\n",
        "    - **AP Test Takers**\n",
        "    - **Total Exams Taken**\n",
        "    - **Number of Exams with scores 3 4 or 5**\n",
        "- Display the column types using the **dtypes** attribute."
      ]
    },
    {
      "metadata": {
        "id": "_qW0QDiM8yFz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8sB0lAzH8yF0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.11 Left, Right, Inner, and Outer Joins"
      ]
    },
    {
      "metadata": {
        "id": "Vjm8aBE68yF1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before we merge our data, we'll need to decide on the merge strategy we want to use. We'll be using the pandas [pandas.DataFrame.merge()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) function, which supports four types of joins -- **left**, **right**, **inner**, and **outer**. Each of these join types dictates how pandas combines the rows.\n",
        "\n",
        "We'll be using the **DBN** column to identify matching rows across data sets. In other words, the values in that column will help us know which row from the first data set to combine with which row in the second data set.\n",
        "\n",
        "There may be **DBN** values that exist in one data set but not in another. This is partly because the data is from different years. Each data set also has inconsistencies in terms of how it was gathered. Human error (and other types of errors) may also play a role. Therefore, we may not find matches for the **DBN** values in **sat_results** in all of the other data sets, and other data sets may have **DBN** values that don't exist in **sat_results**.\n",
        "\n",
        "We'll merge two data sets at a time. For example, we'll merge **sat_results** and **hs_directory**, then merge the result with **ap_2010**, then merge the result of that with **class_size**. We'll continue combining data sets in this way until we've merged all of them. Afterwards, we'll have roughly the same number of rows, but each row will have columns from all of the data sets.\n",
        "\n",
        "The merge strategy we pick will affect the number of rows we end up with. Let's take a look at each strategy.\n",
        "\n",
        "Let's say we're merging the following two data sets:\n",
        "\n",
        "<left><img width=\"300\" src=\"https://drive.google.com/uc?export=view&id=1Vlypix_SIkxCdRS0ABvO4tGiuvFLg321\"></left>\n",
        "\n",
        "With an **inner merge**, we'd only combine rows where the same **DBN** exists in both data sets. We'd end up with this result:\n",
        "\n",
        "<left><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1LR4c8louX-JAZFYta_Y99FLsGkCf9grr\"></left>\n",
        "\n",
        "With a **left merge**, we'd only use **DBN** values from the dataframe on the \"left\" of the merge. In this case, **sat_results** is on the left. Some of the DBNs in **sat_results** don't exist in **class_size**, though. The merge will handle this by assiging null values to the columns in **sat_results** that don't have corresponding data in **class_size.**\n",
        "\n",
        "<left><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1hPoJ5wLECEzz25jrTP5bw9oZ0eerNi2p\"></left>\n",
        "\n",
        "With a **right merge**, we'll only use **DBN** values from the dataframe on the \"right\" of the merge. In this case, **class_size** is on the right:\n",
        "\n",
        "<left><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1YYdf4iEMtHYqRBMTEFcfuTyu9zdFlnx7\"></left>\n",
        "\n",
        "With an outer merge, we'll take any DBN values from either sat_results or class_size:\n",
        "\n",
        "<left><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1sl5wCK3WZ3lTzJm8JUn-bg4MoXl3xSe9\"></left>\n",
        "\n",
        "As you can see, each merge strategy has its advantages. Depending on the strategy we choose, we may preserve rows at the expense of having more missing column data, or minimize missing data at the expense of having fewer rows. Choosing a merge strategy is an important decision; it's worth thinking about your data carefully, and what trade-offs you're willing to make.\n",
        "\n",
        "Because this project is concerned with determing demographic factors that correlate with SAT score, we'll want to preserve as many rows as possible from **sat_results** while minimizing null values.\n",
        "\n",
        "This means that we may need to use different merge strategies with different data sets. Some of the data sets have a lot of missing **DBN** values. This makes a **left** join more appropriate, because we don't want to lose too many rows when we merge. If we did an **inner** join, we would lose the data for many high schools.\n",
        "\n",
        "Some data sets have **DBN** values that are almost identical to those in **sat_results**. Those data sets also have information we need to keep. Most of our analysis would be impossible if a significant number of rows was missing from **demographics**, for example. Therefore, we'll do an inner join to avoid missing data in these columns."
      ]
    },
    {
      "metadata": {
        "id": "XpPfMAwI8yF1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  1.12 Performing the Left Joins"
      ]
    },
    {
      "metadata": {
        "id": "TC5oEhTL8yF2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Both the **ap_2010** and the **graduation** data sets have many missing **DBN** values, so we'll use a left join when we merge the **sat_results** data set with them. Because we're using a **left** join, our final dataframe will have all of the same **DBN** values as the original **sat_results** dataframe.\n",
        "\n",
        "We'll need to use the pandas **df.merge()** method to merge dataframes. The \"left\" dataframe is the one we call the method on, and the \"right\" dataframe is the one we pass into **df.merge()**.\n",
        "\n",
        "Because we're using the **DBN** column to join the dataframes, we'll need to specify the keyword argument **on=\"DBN\"** when calling **pandas.DataFrame.merge().**\n",
        "\n",
        "First, we'll assign **data[\"sat_results\"]** to the variable **combined**. Then, we'll merge all of the other dataframes with **combined**. When we're finished, **combined** will have all of the columns from all of the data sets.\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
        "\n",
        "- Use the pandas [pandas.DataFrame.merge()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) method to merge the **ap_2010** data set into **combined.**\n",
        "    - Make sure to specify **how=\"left\"** as a keyword argument to indicate the correct join type.\n",
        "    - Make sure to assign the result of the merge operation back to **combined.**\n",
        "- Use the pandas **df.merge()** method to merge the **graduation** data set into **combined.**\n",
        "    - Make sure to specify **how=\"left\"** as a keyword argument to get the correct join type.\n",
        "    - Make sure to assign the result of the merge operation back to **combined.**\n",
        "- Display the first few rows of **combined** to verify that the correct operations occurred.\n",
        "- Use the [pandas.DataFrame.shape](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.shape.html) attribute to display the shape of the dataframe and see how many rows now exist."
      ]
    },
    {
      "metadata": {
        "id": "06arIDoX8yF3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VMMJAmet8yF6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.13 Performing the Inner Joins"
      ]
    },
    {
      "metadata": {
        "id": "Nn8-wD4k8yF7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we've performed the left joins, we still have to merge **class_size**, **demographics**, **survey**, and **hs_directory** into **combined**. Because these files contain information that's more valuable to our analysis and also have fewer missing **DBN** values, we'll use the **inner** join type.\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
        "\n",
        "- Merge **class_size** into **combined**. Then, merge **demographics**, **survey**, and **hs_directory** into **combined** one by one, in that order.\n",
        "    - Be sure to follow the exact order above.\n",
        "    - Remember to specify the correct column to join on, as well as the correct join type.\n",
        "- Display the first few rows of **combined** to verify that the correct operations occurred.\n",
        "- Call **pandas.DataFrame.shape()** to display the shape of the dataframe to see how many rows now exist."
      ]
    },
    {
      "metadata": {
        "id": "xEbfC4f58yF8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LoZeRh6W8yF_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  1.14 Filling in Missing Values"
      ]
    },
    {
      "metadata": {
        "id": "G_P59inq8yF_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You may have noticed that the inner joins resulted in 116 fewer rows in **sat_results**. This is because pandas couldn't find the **DBN** values that existed in **sat_results** in the other data sets. While this is worth investigating, we're currently looking for high-level correlations, so we don't need to dive into which **DBNs** are missing.\n",
        "\n",
        "You may also have noticed that we now have many columns with null (**NaN**) values. This is because we chose to do **left** joins, where some columns may not have had data. The data set also had some missing values to begin with. If we hadn't performed a **left** join, all of the rows with missing data would have been lost in the merge process, which wouldn't have left us with many high schools in our data set.\n",
        "\n",
        "There are several ways to handle missing data, and we'll cover them in more detail later on. For now, we'll just fill in the missing values with the overall mean for the column, like so:\n",
        "\n",
        "<left><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1OmhXzMuPrGSmyyugGXpmrRlLznDHxOeT\"></left>\n",
        "\n",
        "In the diagram above, the mean of the first column is (1800 + 1600 + 2200 + 2300) / 4, or 1975, and the mean of the second column is (20 + 30 + 30 + 50) / 4, or 32.5. We replace the missing values with the means of their respective columns, which allows us to proceed with analyses that can't handle missing values (like correlations).\n",
        "\n",
        "We can fill in missing data in pandas using the [pandas.DataFrame.fillna()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) method. This method will replace any missing values in a dataframe with the values we specify. We can compute the mean of every column using the [pandas.DataFrame.mean()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mean.html) method. If we pass the results of the **df.mean()** method into the **df.fillna()** method, pandas will fill in the missing values in each column with the mean of that column.\n",
        "\n",
        "Here's an example of how we would accomplish this:\n",
        "\n",
        "```python\n",
        "means = df.mean()\n",
        "df = df.fillna(means)\n",
        "```\n",
        "\n",
        "Note that if a column consists entirely of null or **NaN** values, pandas won't be able to fill in the missing values when we use the **df.fillna()** method along with the **df.mean()** method, because there won't be a mean.\n",
        "\n",
        "We should fill any **NaN** or null values that remain after the initial replacement with the value 0. We can do this by passing 0 into the **df.fillna()** method.\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
        "\n",
        "- Calculate the means of all of the columns in **combined** using the **pandas.DataFrame.mean()** method.\n",
        "- Fill in any missing values in **combined** with the means of the respective columns using the **pandas.DataFrame.fillna()** method.\n",
        "- Fill in any remaining missing values in **combined** with 0 using the **df.fillna()** method.\n",
        "- Display the first few rows of **combined** to verify that the correct operations occurred."
      ]
    },
    {
      "metadata": {
        "id": "XLJ8giWa8yGA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TkFLrp0r8yGG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.15 Adding a School District Column for Mapping"
      ]
    },
    {
      "metadata": {
        "id": "Bhwo7emM8yGG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We've finished cleaning and combining our data! We now have a clean data set on which we can base our analysis. Mapping the statistics out on a school district level might be an interesting way to analyze them. Adding a column to the data set that specifies the school district will help us accomplish this.\n",
        "\n",
        "The school district is just the first two characters of the **DBN**. We can apply a function over the **DBN** column of **combined** that pulls out the first two letters.\n",
        "\n",
        "For example, we can use indexing to extract the first few characters of a string, like this:\n",
        "\n",
        "```python\n",
        "name = \"Sinbad\"\n",
        "print(name[0:2])\n",
        "```\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "<left><img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n",
        "\n",
        "- Write a function that extracts the first two characters of a string and returns them.\n",
        "- Apply the function to the **DBN** column of **combined**, and assign the result to the **school_dist** column of **combined**.\n",
        "- Display the first few items in the **school_dist** column of **combined** to verify the results.\n"
      ]
    },
    {
      "metadata": {
        "id": "gIk75azA8yGG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8407VxT8yGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.16 Next Steps"
      ]
    },
    {
      "metadata": {
        "id": "0YOOKVLg8yGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now have a clean data set we can analyze! We've done a lot in this mission. We've gone from having several messy sources to one clean, combined, data set that's ready for analysis.\n",
        "\n",
        "Along the way, we've learned about:\n",
        "\n",
        "- How to handle missing values\n",
        "- Different types of merges\n",
        "- How to condense data sets\n",
        "- How to compute averages across dataframes\n",
        "\n",
        "Data scientists rarely start out with tidy data sets, which makes cleaning and combining them one of the most critical skills any data professional can learn.\n"
      ]
    }
  ]
}